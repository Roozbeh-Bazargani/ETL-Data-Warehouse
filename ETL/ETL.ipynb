{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from optparse import OptionParser, OptionGroup\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an existing database\n",
    "conn1 = psycopg2.connect(\"dbname=DB-project user=postgres password=admin\")\n",
    "conn2 = psycopg2.connect(\"dbname=DB-project-ETL user=postgres password=admin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur1 = conn1.cursor()\n",
    "cur2 = conn2.cursor()\n",
    "\n",
    "conn1.autocommit = True\n",
    "conn2.autocommit = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pk(cursor, table):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT c.column_name, c.data_type\n",
    "    FROM information_schema.table_constraints tc \n",
    "    JOIN information_schema.constraint_column_usage AS ccu USING (constraint_schema, constraint_name) \n",
    "    JOIN information_schema.columns AS c ON c.table_schema = tc.constraint_schema\n",
    "      AND tc.table_name = c.table_name AND ccu.column_name = c.column_name\n",
    "    WHERE constraint_type = 'PRIMARY KEY' and tc.table_name = '{table}';\n",
    "    \"\"\".format(table=table))\n",
    "\n",
    "def extract_pk_values(cursor, table, pk):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT {pk}\n",
    "    FROM {table}\n",
    "    \"\"\".format(table=table, pk=pk))\n",
    "    \n",
    "def table_allrows(cursor, table):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT *\n",
    "    FROM {table};\n",
    "    \"\"\".format(table=table))\n",
    "\n",
    "def insert_warehouse(cursor, value, table, pk, pk_value, attributes):\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO {table}\n",
    "    VALUES {value}\n",
    "    ON CONFLICT ({pk}) DO\n",
    "    UPDATE\n",
    "    SET {attributes} = {value};\n",
    "    \"\"\".format(table=table, value=value, pk=pk, pk_value=pk_value, attributes=attributes))\n",
    "    \n",
    "def delete_warehouse(cursor, table, pk, pk_values):\n",
    "    query = \"\"\"\n",
    "    DELETE FROM {table}\n",
    "    \"\"\".format(table=table)\n",
    "    \n",
    "    if pk_values!=')': # data in the table\n",
    "        query += \"\"\"WHERE {pk} NOT IN {pk_values}\n",
    "        \"\"\".format(pk=pk, pk_values=pk_values)\n",
    "        \n",
    "    cursor.execute(query)\n",
    "    \n",
    "def record_value(records):\n",
    "    pk_value = records[0]\n",
    "    if type(pk_value) == str:\n",
    "        pk_value = \"'\" + pk_value + \"'\"\n",
    "    value = '(' + str(pk_value)\n",
    "    for record in records[1:]:\n",
    "        if record == None:\n",
    "            record = 'NULL'\n",
    "        elif type(record) != int:\n",
    "            record = \"'{}'\".format(record)\n",
    "        value += ', ' + str(record)\n",
    "    value += ')'\n",
    "    if len(records) == 1:\n",
    "        value = value[:-1] + ')'\n",
    "    return pk_value, value\n",
    "\n",
    "def key_pk_values(keys):\n",
    "    pk_values = '('\n",
    "    for key in keys:\n",
    "        if type(key[0]) == str:\n",
    "            pk_values += \"'{}', \".format(key[0])\n",
    "        else:\n",
    "            pk_values += '{}, '.format(key[0])\n",
    "    pk_values = pk_values[:-2]\n",
    "    pk_values +=')'\n",
    "    return pk_values\n",
    "\n",
    "def columns(cursor, table):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE TABLE_NAME = '{table}'\n",
    "    ORDER BY ORDINAL_POSITION\n",
    "    \"\"\".format(table=table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_insert(table):\n",
    "    # insert and update \n",
    "    with conn1.cursor() as cursor1:\n",
    "        table_allrows(cursor1, table)\n",
    "        records = cursor1.fetchone()\n",
    "        with conn2.cursor() as cursor2:\n",
    "            extract_pk(cursor2, table)\n",
    "            pk = cursor2.fetchone()[0]\n",
    "        while records != None:\n",
    "            pk_value, value = record_value(records)\n",
    "            with conn1.cursor() as cursor11:\n",
    "                columns(cursor11, table)\n",
    "                cols = cursor11.fetchall()\n",
    "                if len(cols) == 1:\n",
    "                    attributes = cols[0][0]\n",
    "                else:\n",
    "                    attributes = '('\n",
    "                    for col in cols:\n",
    "                        attributes += col[0] + ', '\n",
    "                    attributes = attributes[:-2] + ')'\n",
    "                with conn2.cursor() as cursor2:\n",
    "                    try:\n",
    "                        insert_warehouse(cursor2, value, table, pk, pk_value, attributes)\n",
    "                        conn2.commit()\n",
    "                    except Exception as e:\n",
    "                        conn2.rollback()\n",
    "                        raise\n",
    "            records = cursor1.fetchone()\n",
    "\n",
    "\n",
    "def pipeline_delete(table):\n",
    "    # delete\n",
    "    with conn2.cursor() as cursor2:\n",
    "        extract_pk(cursor2, table)\n",
    "        pk = cursor2.fetchone()[0]\n",
    "        with conn1.cursor() as cursor1:\n",
    "            extract_pk_values(cursor1, table, pk)\n",
    "            keys = cursor1.fetchall()\n",
    "            pk_values = key_pk_values(keys)\n",
    "            with conn2.cursor() as cursor22:\n",
    "                delete_warehouse(cursor22, table, pk, pk_values)\n",
    "                conn2.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_insert('borrowed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_delete('genre_book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writedeps(cursor, tbl, dependency):\n",
    "    sql = \"\"\"SELECT\n",
    "        tc.constraint_name, tc.table_name, kcu.column_name,\n",
    "        ccu.table_name AS foreign_table_name,\n",
    "        ccu.column_name AS foreign_column_name\n",
    "    FROM\n",
    "        information_schema.table_constraints AS tc\n",
    "    JOIN information_schema.key_column_usage AS kcu ON\n",
    "        tc.constraint_name = kcu.constraint_name\n",
    "    JOIN information_schema.constraint_column_usage AS ccu ON\n",
    "        ccu.constraint_name = tc.constraint_name\n",
    "    WHERE constraint_type = 'FOREIGN KEY' AND tc.table_name = '%s'\"\"\"\n",
    "    cursor.execute(sql % tbl)\n",
    "    for row in cursor.fetchall():\n",
    "        constraint, table, column, foreign_table, foreign_column = row\n",
    "        print('{} -> {} [label={}];'.format(tbl, foreign_table, constraint))\n",
    "        dependency.append([tbl, foreign_table])\n",
    "    return dependency\n",
    "\n",
    "\n",
    "def get_tables(cursor):\n",
    "    cursor.execute(\"SELECT tablename FROM pg_tables WHERE schemaname='public'\")\n",
    "    for row in cursor.fetchall():\n",
    "        yield row[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digraph F {\n",
      "\n",
      "ranksep=1.0; size=\"18.5, 15.5\"; rankdir=LR;\n",
      "book -> person [label=book_main_author_fkey];\n",
      "book -> languageb [label=book_main_language_fkey];\n",
      "book -> genre [label=book_main_genre_fkey];\n",
      "user_library -> person [label=user_library_person_id_fkey];\n",
      "written_by -> person [label=written_by_person_id_fkey];\n",
      "written_by -> book [label=written_by_book_id_fkey];\n",
      "translated_by -> person [label=translated_by_person_id_fkey];\n",
      "translated_by -> book [label=translated_by_book_id_fkey];\n",
      "language_book -> languageb [label=language_book_languageb_fkey];\n",
      "language_book -> book [label=language_book_book_id_fkey];\n",
      "genre_book -> genre [label=genre_book_genre_fkey];\n",
      "genre_book -> book [label=genre_book_book_id_fkey];\n",
      "borrowed -> user_library [label=borrowed_user_id_fkey];\n",
      "borrowed -> book [label=borrowed_book_id_fkey];\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Digraph F {\\n\")\n",
    "print('ranksep=1.0; size=\"18.5, 15.5\"; rankdir=LR;')\n",
    "dependency = []\n",
    "with conn1.cursor() as cursor1:\n",
    "    for i in get_tables(cursor1):\n",
    "        dependency = writedeps(cursor1, i, dependency)\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['book', 'person'], ['book', 'languageb'], ['book', 'genre'], ['user_library', 'person'], ['written_by', 'person'], ['written_by', 'book'], ['translated_by', 'person'], ['translated_by', 'book'], ['language_book', 'languageb'], ['language_book', 'book'], ['genre_book', 'genre'], ['genre_book', 'book'], ['borrowed', 'user_library'], ['borrowed', 'book']]\n"
     ]
    }
   ],
   "source": [
    "print(dependency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tables:\n",
      " ['person', 'book', 'languageb', 'genre', 'user_library', 'written_by', 'translated_by', 'language_book', 'genre_book', 'borrowed']\n"
     ]
    }
   ],
   "source": [
    "def tables(cursor):\n",
    "    cursor.execute(\"SELECT tablename FROM pg_tables WHERE schemaname='public'\")\n",
    "    tables = []\n",
    "    for row in cursor.fetchall():\n",
    "        tables.append(row[0])\n",
    "    return tables\n",
    "\n",
    "with conn1.cursor() as cursor1:\n",
    "    tables = tables(cursor1)\n",
    "    print('list of tables:\\n', tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tables.index('book'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is a Topological Sort of the given graph\n",
      "[9, 8, 7, 6, 5, 4, 1, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#Python program to print topological sorting of a DAG\n",
    "from collections import defaultdict\n",
    "\n",
    "#Class to represent a graph\n",
    "class Graph:\n",
    "    def __init__(self,vertices):\n",
    "        self.graph = defaultdict(list) #dictionary containing adjacency List\n",
    "        self.V = vertices #No. of vertices\n",
    "\n",
    "    # function to add an edge to graph\n",
    "    def addEdge(self,u,v):\n",
    "        self.graph[u].append(v)\n",
    "\n",
    "    # A recursive function used by topologicalSort\n",
    "    def topologicalSortUtil(self,v,visited,stack):\n",
    "\n",
    "        # Mark the current node as visited.\n",
    "        visited[v] = True\n",
    "\n",
    "        # Recur for all the vertices adjacent to this vertex\n",
    "        for i in self.graph[v]:\n",
    "            if visited[i] == False:\n",
    "                self.topologicalSortUtil(i,visited,stack)\n",
    "\n",
    "        # Push current vertex to stack which stores result\n",
    "        stack.insert(0,v)\n",
    "\n",
    "    # The function to do Topological Sort. It uses recursive\n",
    "    # topologicalSortUtil()\n",
    "    def topologicalSort(self):\n",
    "        # Mark all the vertices as not visited\n",
    "        visited = [False]*self.V\n",
    "        stack =[]\n",
    "\n",
    "        # Call the recursive helper function to store Topological\n",
    "        # Sort starting from all vertices one by one\n",
    "        for i in range(self.V):\n",
    "            if visited[i] == False:\n",
    "                self.topologicalSortUtil(i,visited,stack)\n",
    "\n",
    "        # Print contents of stack\n",
    "#         stack = stack[::-1]\n",
    "        print(stack)\n",
    "        return stack\n",
    "\n",
    "g= Graph(len(tables))\n",
    "for dep in dependency:\n",
    "    g.addEdge(tables.index(dep[0]), tables.index(dep[1]))\n",
    "\n",
    "print(\"Following is a Topological Sort of the given graph\")\n",
    "stack = g.topologicalSort()\n",
    "#This code is contributed by Neelam Yadav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_to_sort_tables(stack, tables):\n",
    "    sort_table = []\n",
    "    for s in stack:\n",
    "        sort_table.append(tables[s])\n",
    "    return sort_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borrowed', 'genre_book', 'language_book', 'translated_by', 'written_by', 'user_library', 'book', 'genre', 'languageb', 'person']\n"
     ]
    }
   ],
   "source": [
    "sort_table = stack_to_sort_tables(stack, tables)\n",
    "print(sort_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att:\n",
      " (person_id, last_name, first_name)\n",
      "att:\n",
      " genre\n",
      "att:\n",
      " genre\n",
      "att:\n",
      " (user_id, person_id, birth_date, register_date, register_number, telephone, address)\n"
     ]
    }
   ],
   "source": [
    "for table in sort_table:\n",
    "    pipeline_delete(table)\n",
    "\n",
    "for table in sort_table[::-1]:\n",
    "    pipeline_insert(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
